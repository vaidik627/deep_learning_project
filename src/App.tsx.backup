import React, { useState, useEffect, useCallback, useMemo } from 'react';
import './App.css';
import ParticleBackground from './components/ParticleBackground';
import AnimatedGradientBackground from './components/AnimatedGradientBackground';
import Sidebar from './components/Sidebar';
import ChatInput from './components/ChatInput';
import ModelCardContainer from './components/ModelCardContainer';
import { simulateStreamingResponse } from './utils/streamResponse';
import { fetchModelResponse, initializeNvidiaAPI } from './utils/api';
import { throttle } from './utils/throttle';

// Types
interface ChatMessage {
  user: string;
  ai: string;
  timestamp: number;
}

interface ChatSession {
  id: string;
  title: string;
  timestamp: number;
  modelsUsed: string[];
  messages: Record<string, ChatMessage[]>;
}

// Define model data
const modelData = [
  { id: 'gpt', name: 'NVIDIA Kimi Instruct', color: '#00ff8f', icon: 'ü§ñ' },
  { id: 'gemini', name: 'Gemini 2.5 Pro', color: '#b366ff', icon: 'üîÆ' },
  { id: 'deepseek', name: 'DeepSeek Chat', color: '#ff6b00', icon: 'üß†' },
  { id: 'perplexity', name: 'Perplexity Search', color: '#00ffc6', icon: 'üîç' },
  { id: 'anthropic', name: 'Claude 3.5 Sonnet', color: '#ff9500', icon: 'üìù' },
  { id: 'mistral', name: 'Mistral Large', color: '#ff4d94', icon: 'üå™Ô∏è' },
];

// Mock responses for different models
const mockResponses: Record<string, string[]> = {
  gpt: [
    "I'm NVIDIA Kimi Instruct, an advanced language model. I can help you with a wide range of tasks including writing, analysis, coding, and creative problem-solving. How can I assist you today?",
    "Based on your query, I'd recommend breaking this down into smaller, manageable steps. Let me provide a structured approach to help you achieve your goal effectively.",
    "That's an interesting question! Let me analyze this from multiple perspectives to give you a comprehensive answer."
  ],
  gemini: [
    "Hello! I'm Gemini 2.5 Pro. I excel at multimodal understanding and can process both text and visual information. I'm here to provide detailed, accurate responses to your questions.",
    "I've processed your request and here's my analysis: This requires a balanced approach considering both technical feasibility and practical implementation.",
    "Great question! Let me break this down with clear examples and actionable insights."
  ],
  deepseek: [
    "DeepSeek Chat here! I specialize in deep reasoning and technical problem-solving. I can help you understand complex concepts and provide detailed technical explanations.",
    "Let me think about this systematically. Your question involves several interconnected factors that we should consider separately.",
    "Based on my analysis, here's a comprehensive explanation with the key technical details you need to know."
  ],
  perplexity: [
    "Perplexity Search at your service! I'm designed to find and synthesize information from across the web. Let me search for the most relevant and up-to-date information for you.",
    "I've searched multiple sources and found some interesting insights on your question. Here's what the latest research and data suggest.",
    "According to recent information, there are several perspectives on this topic. Let me present the most relevant findings for you."
  ],
  anthropic: [
    "Hello, I'm Claude 3.5 Sonnet. I'm designed to be helpful, harmless, and honest. I excel at thoughtful analysis and nuanced responses. How can I assist you today?",
    "That's a thoughtful question. Let me explore this topic with care and provide a balanced perspective that considers different viewpoints.",
    "I appreciate your question. Let me think about this carefully and provide a response that addresses the key aspects while acknowledging the nuances involved."
  ],
  mistral: [
    "Mistral Large here. I'm optimized for efficiency and accuracy across a wide range of tasks. How can I help you today?",
    "Let me address your question directly. Based on my analysis, here are the key points to consider.",
    "I'll provide a concise but comprehensive answer to your question, focusing on the most relevant information."
  ]
};

function App() {
  // State for user message
  const [userMessage, setUserMessage] = useState('');
  const [isFirstPrompt, setIsFirstPrompt] = useState(true);
  
  // Model states
  const [enabledModels, setEnabledModels] = useState<string[]>(['gpt', 'gemini']);
  const [typingModels, setTypingModels] = useState<string[]>([]);
  
  // NEW: Stacked conversation history per model
  const [modelConversations, setModelConversations] = useState<Record<string, ChatMessage[]>>({});
  
  // Chat history state
  const [chatSessions, setChatSessions] = useState<ChatSession[]>([]);
  const [currentSessionId, setCurrentSessionId] = useState<string | null>(null);
  
  // Initialize NVIDIA API
  useEffect(() => {
    initializeNvidiaAPI().then((verified: boolean) => {
      if (verified) {
        console.log('‚úÖ NVIDIA API initialized successfully');
      } else {
        console.warn('‚ö†Ô∏è NVIDIA API verification failed - will use fallback');
      }
    });
  }, []);
  
  // Load chat history from localStorage on mount
  useEffect(() => {
    const savedSessions = localStorage.getItem('chatSessions');
    if (savedSessions) {
      try {
        const sessions = JSON.parse(savedSessions);
        setChatSessions(sessions);
      } catch (e) {
        console.error('Failed to load chat sessions:', e);
      }
    }
  }, []);
  
  // Save chat sessions to localStorage whenever they change
  useEffect(() => {
    if (chatSessions.length > 0) {
      localStorage.setItem('chatSessions', JSON.stringify(chatSessions));
    }
  }, [chatSessions]);
  
  // Toggle model enabled/disabled
  const handleToggleModel = useCallback((modelId: string) => {
    setEnabledModels(prev => {
      if (prev.includes(modelId)) {
        return prev.filter(id => id !== modelId);
      } else {
        return [...prev, modelId];
      }
    });
  }, []);
  
  // Load a session
  const handleLoadSession = useCallback((sessionId: string) => {
    const session = chatSessions.find(s => s.id === sessionId);
    if (session) {
      setCurrentSessionId(sessionId);
      setModelConversations(session.messages || {});
      setIsFirstPrompt(false);
    }
  }, [chatSessions]);
  
  // Create a new session
  const handleNewSession = useCallback(() => {
    setCurrentSessionId(null);
    setModelConversations({});
    setUserMessage('');
    setIsFirstPrompt(true);
  }, []);
  
  // Delete a session
  const handleDeleteSession = useCallback((sessionId: string) => {
    setChatSessions(prev => prev.filter(s => s.id !== sessionId));
    if (currentSessionId === sessionId) {
      handleNewSession();
    }
  }, [currentSessionId, handleNewSession]);
  
  // Prepare models data for ModelCardContainer
  const modelsForCards = useMemo(() => {
    return modelData.map(model => ({
      ...model,
      isTyping: typingModels.includes(model.id),
      conversation: modelConversations[model.id] || []
    }));
  }, [modelData, typingModels, modelConversations]);
  
  // Handle chat submission with optimized streaming
  const handleChatSubmit = useCallback(async (message: string) => {
    setUserMessage(message);
    setIsFirstPrompt(false);
    
    // Create or update session
    const sessionId = currentSessionId || `session_${Date.now()}`;
    const sessionTitle = message.split(' ').slice(0, 6).join(' ') + (message.split(' ').length > 6 ? '...' : '');
    
    if (!currentSessionId) {
      setCurrentSessionId(sessionId);
    }
    
    // Set typing state for all enabled models
    setTypingModels(enabledModels.slice());
    
    // Initialize conversations with user message
    enabledModels.forEach(modelId => {
      setModelConversations(prev => ({
        ...prev,
        [modelId]: [
          ...(prev[modelId] || []),
          { user: message, ai: "Generating...", timestamp: Date.now() }
        ]
      }));
    });
    
    // Execute all model requests in parallel (async) with streaming
    const modelPromises = enabledModels.map(async (modelId) => {
      try {
        // Create throttled streaming callback for better performance
        const updateStream = throttle((partialText: string) => {
          setModelConversations(prev => {
            const currentConversation = prev[modelId] || [];
            const updatedConversation = [...currentConversation];
            
            // Update the last message with streaming text
            if (updatedConversation.length > 0) {
              updatedConversation[updatedConversation.length - 1] = {
                user: message,
                ai: partialText,
                timestamp: Date.now()
              };
            }
            
            return {
              ...prev,
              [modelId]: updatedConversation
            };
          });
        }, 50); // Throttle to 50ms for smoother UI
        
        // Use real NVIDIA API for GPT-5 Nano, mock for others
        if (modelId === 'gpt') {
          // Real NVIDIA API call
          await fetchModelResponse(modelId, message, updateStream);
        } else {
          // Mock response for other models
          const typingDuration = Math.random() * 2000 + 1500;
          await new Promise(resolve => setTimeout(resolve, typingDuration));
          
          const responses = mockResponses[modelId] || ["I'm processing your request..."];
          const randomResponse = responses[Math.floor(Math.random() * responses.length)];
          
          await simulateStreamingResponse(modelId, randomResponse, updateStream, 15);
        }
        
        // Remove from typing state
        setTypingModels(prev => prev.filter(id => id !== modelId));
        
        return { modelId, success: true };
      } catch (error) {
        console.error(`${modelId} failed:`, error);
        
        // Update with error message
        setModelConversations(prev => {
          const currentConversation = prev[modelId] || [];
          const updatedConversation = [...currentConversation];
          
          if (updatedConversation.length > 0) {
            updatedConversation[updatedConversation.length - 1] = {
              user: message,
              ai: `‚ö†Ô∏è Error: ${error instanceof Error ? error.message : String(error)}`,
              timestamp: Date.now()
            };
          }
          
          return {
            ...prev,
            [modelId]: updatedConversation
          };
        });
        
        setTypingModels(prev => prev.filter(id => id !== modelId));
        return { modelId, success: false };
      }
    });
    
    // Wait for all models to complete
    await Promise.allSettled(modelPromises);
    
    // Update session with latest conversations
    setChatSessions(prev => {
      const existingIndex = prev.findIndex(s => s.id === sessionId);
      const newSession = { 
        id: sessionId, 
        title: sessionTitle, 
        timestamp: Date.now(), 
        modelsUsed: enabledModels,
        messages: modelConversations
      };
      
      if (existingIndex >= 0) {
        const updated = [...prev];
        updated[existingIndex] = newSession;
        return updated;
      }
      
      return [newSession, ...prev];
    });
  }, [currentSessionId, enabledModels, modelConversations]);
  
  return (
    <div className="flex flex-col min-h-screen bg-background text-white overflow-hidden">
      {/* Background Effects - optimized */}
      <div className="fixed inset-0 z-0">
        <ParticleBackground intensity="low" />
        <AnimatedGradientBackground />
      </div>
      
      {/* Sidebar */}
      <Sidebar 
        isExpanded={true}
        toggleSidebar={() => {}}
        chatSessions={chatSessions}
        currentSessionId={currentSessionId}
        onLoadSession={handleLoadSession}
        onNewChat={handleNewSession}
        onClearHistory={() => {}}
      />
      
      {/* Main Content */}
      <main className="flex-1 ml-[260px] transition-all duration-300 relative">
        {/* Active Models Info */}
        <div className="p-4 pb-0">
          <p className="text-gray-400 text-sm mb-3">
            Active Models: {enabledModels.map(id => modelData.find(m => m.id === id)?.name).join(", ")}
          </p>
        </div>
        
        {/* Model Cards Container */}
        <div className="p-4 pt-0 model-grid">
          <ModelCardContainer 
            models={modelsForCards}
            enabledModels={enabledModels}
            onToggleModel={handleToggleModel}
          />
        </div>
        
        {/* Chat Input */}
        <ChatInput 
          onSubmit={handleChatSubmit}
          isFirstPrompt={isFirstPrompt}
          setIsFirstPrompt={setIsFirstPrompt}
        />
      </main>
    </div>
  );
}

export default App;
